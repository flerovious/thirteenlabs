{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_hub.youtube_transcript import YoutubeTranscriptReader\n",
    "\n",
    "loader = YoutubeTranscriptReader()\n",
    "documents = loader.load_data(ytlinks=['https://www.youtube.com/watch?v=i3OYlaoj-BM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/flerovious/Library/Caches/llama_index...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine import CitationQueryEngine\n",
    "\n",
    "query_engine = CitationQueryEngine.from_args(\n",
    "    index,\n",
    "    similarity_top_k=3,\n",
    "    # here we can control how granular citation sources are, the default is 512\n",
    "    citation_chunk_size=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is zero shot object detection?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='Zero shot object detection refers to the ability of a model to detect and localize objects in a new domain without the need for fine-tuning on data from that domain [1]. This means that the model can be applied to a new domain without any prior training on data from that domain [1].', source_nodes=[NodeWithScore(node=TextNode(id_='8ad20062-4699-4e04-ad71-2c9afb1b7318', embedding=None, metadata={'video_id': 'i3OYlaoj-BM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='43db68bf-51a9-4396-bd92-a9c0859f021c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'i3OYlaoj-BM'}, hash='41a8b6cb1eac4d98a00cec03a94ef4b03386e74e3c5f3144504b87d29f265346'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='244cd470-997b-43da-8fb4-3d3a925d8ae2', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'i3OYlaoj-BM'}, hash='354567c573111a839ce8641cbd63ba8094a26d31fb055e1912a96515e1b346b8')}, hash='7ce6ea1f6d225b87e0d048c77fe7064a50393e2bfcfd6e2f3cac4937d558d8a6', text=\"Source 1:\\nthe imaginative laws scale visual\\nrecognition challenge was a\\nworld-changing competition\\nthat ran from around 2010 to 2017.\\nduring his time the competition acted as\\nthe place to go if you needed to find\\nwhat the current state of the art was in\\nimage classification object localization\\nobject detection as well as that\\n2012 onwards it really acted as the\\nCatalyst of the explosion in deep\\nlearning researchers fine-tuned better\\nperforming computer vision models year\\non year but there was a unquestioned\\nassumption causing problems\\nit was assumed that every new task\\nrequired model fine tuning this required\\na lot of data and a lot of data required\\na lot of capital and time it wasn't\\nuntil recently that this assumption was\\nchallenged and proven wrong the\\nastonishing rise of what are called\\nmulti-modal models has made the what was\\nthought impossible very possible across\\nvarious domains and tasks one of those\\nis called zero shot object detection and\\nlocalization now zero shot refers to\\ntaking a model and applying it to a new\\ndomain without ever fine-tuning it on\\ndata from that new domain so that means\\nwe can take a model who can it maybe it\\nworks in in one domain classification in\\none particular area on one data set and\\nwe can take that same model without any\\nfine tuning and we can use it for object\\ndetection in a completely different\\ndomain without that model seeing any\\ntraining data from that new domain so in\\nthis video we're going to explore how to\\nuse open ai's clip for zero shots object\\ndetection and localization let's begin\\nwith taking a quick look at image\\nclassification now image classification\\ncan kind of be seen as one of the\\nsimplest tasks in visual recognition and\\nit's also the first step on the way to\\nobject detection at his core it's just\\nassigning a categorical label to an\\nimage now moving on from image\\nclassification we have object\\nlocalization object localization is\\nimage classification followed by the\\nidentification of where in the image the\\nspecific object actually is so we are\\nlocalizing the the object now doing that\\nwhere essentially just going to identify\\nthe coordinates on the image I going to\\nreturn the typical approach to this is\\nreturn an image where you have like a\\nbounding box surrounding the the object\\nthat you are looking for and then we\\ntake this one step further to perform\\nobject detection with detection we\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8645808801817435), NodeWithScore(node=TextNode(id_='8ad20062-4699-4e04-ad71-2c9afb1b7318', embedding=None, metadata={'video_id': 'i3OYlaoj-BM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='43db68bf-51a9-4396-bd92-a9c0859f021c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'i3OYlaoj-BM'}, hash='41a8b6cb1eac4d98a00cec03a94ef4b03386e74e3c5f3144504b87d29f265346'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='244cd470-997b-43da-8fb4-3d3a925d8ae2', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'i3OYlaoj-BM'}, hash='354567c573111a839ce8641cbd63ba8094a26d31fb055e1912a96515e1b346b8')}, hash='7ce6ea1f6d225b87e0d048c77fe7064a50393e2bfcfd6e2f3cac4937d558d8a6', text=\"Source 2:\\nare looking for and then we\\ntake this one step further to perform\\nobject detection with detection we are\\nlocalizing multiple objects within the\\nimage or we have the capability to\\nidentify multiple objects within the\\nimage so in this example we have cat and\\na dog we would expect with object\\ndetection to identify both the cat and\\nthe dot in the case of us having\\nmultiple dolbs in this image almost Cuts\\nin this image we would also expect the\\nobject detection algorithm to actually\\nidentify each one of those independently\\nnow in the past if we wanted to switch a\\nmodel between anyone these tasks would\\nhave to fine-tune or more data you want\\nto switch it to another domain we would\\nhave to also fine tune it on new data\\nfrom that domain but that's not always\\nthe case with models like open ai's clip\\nfor performing each one of these tasks\\nin a zero shot setting now open ai's\\nclip is a multi-modal model that has\\nbeen pre-trained on a huge number of\\ntext and image Pairs and it essentially\\nworks by identifying text and image\\npairs that have a similar meaning and\\nplacing them within a similar Vector\\nspace so every text and every image gets\\nconverted into a vector and they are\\nplaced in a shared Vector space and the\\nvectors that appear close together they\\nhave a similar meaning now Clips very\\nbroad pre-training means that it can\\nperform very effectively across a lot of\\ndifferent domains it's seen a lot of\\ndata and so it has a good understanding\\nof all these different things and we can\\neven adjust the task being performed\\nwith just a few code changes we don't\\nactually have to is the model itself we\\njust adjust the code around the model\\nand that's very much thanks to Clips\\nfocus on sort of comparing these vectors\\nso for example for classification we\\ngive clip a list of our plus labels and\\nthen we pass in the images and we just\\nidentify within that space where those\\nimages are with respect to those plus\\nlabel vectors and which plus label is\\nthe most similar to our particular image\\nand then that is our prediction so that\\nmost similar plus label\\nthat's our predated class now for\\nobjects localization we apply a very\\nsimilar type of logic as before we\\ncreate a class label but unlike before\\nwe don't need the entire image into clip\\nto localize an object we have to break\\nthe image into patches we then pass\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8645808801817435), NodeWithScore(node=TextNode(id_='8ad20062-4699-4e04-ad71-2c9afb1b7318', embedding=None, metadata={'video_id': 'i3OYlaoj-BM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='43db68bf-51a9-4396-bd92-a9c0859f021c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'i3OYlaoj-BM'}, hash='41a8b6cb1eac4d98a00cec03a94ef4b03386e74e3c5f3144504b87d29f265346'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='244cd470-997b-43da-8fb4-3d3a925d8ae2', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'i3OYlaoj-BM'}, hash='354567c573111a839ce8641cbd63ba8094a26d31fb055e1912a96515e1b346b8')}, hash='7ce6ea1f6d225b87e0d048c77fe7064a50393e2bfcfd6e2f3cac4937d558d8a6', text='Source 3:\\nimage into clip\\nto localize an object we have to break\\nthe image into patches we then pass a\\nwindow over all of those patches\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8645808801817435), NodeWithScore(node=TextNode(id_='b40c56d0-61d6-41a6-9110-cf1c7628ce82', embedding=None, metadata={'video_id': 'i3OYlaoj-BM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='43db68bf-51a9-4396-bd92-a9c0859f021c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'i3OYlaoj-BM'}, hash='41a8b6cb1eac4d98a00cec03a94ef4b03386e74e3c5f3144504b87d29f265346'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='b2cb23fa-5555-453f-953f-45f02802271c', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'i3OYlaoj-BM'}, hash='1842675aa6bef1f97506b103152667bd886835f15c48842f3c42d388e0b1822b')}, hash='3ac5818e188b921327745a5b8b7c597ffc7675664163f70dfa696a4b4ee50e58', text=\"Source 4:\\n5 like we used before you can see\\nover there we have our patch size we\\njust need to pass to that for a\\ncalculation of the or for the conversion\\nand we have our patch size which we\\npassed to that for the conversion from\\npatch\\npixel from patch coordinates to pixel\\ncoordinates now we also have our scores\\nand that will return the minimum X and Y\\ncoordinates and also width and height of\\nthe box\\nwe create the bounding box\\nnow we add that to the axis okay so now\\nlet's visualize all this see what we get\\nso here I've used a slightly smaller\\nwindow size before using six just to\\npoint out that you can change it and\\ndepending on your image it may be better\\nto use a smaller or larger window\\nand you can see so what we're doing here\\nwe've got a cat and a butterfly and you\\ncan see that we get we get a butterfly\\nhere and we get the cat here okay it's\\npretty cool and like I said with clip we\\ncan apply this object detection without\\nfine tuning all we need to do is change\\nthese prompts here\\nokay so it's it's really straightforward\\nto modify this and move it to a new\\ndomain okay so that's it for this\\nwalkthrough of object localization and\\nobject detection with clip as I said I\\nthink zero shot object localization\\ndetection and even classification opens\\nthe doors to a lot of projects and use\\ncases that were just not accessible\\nbefore because time and capital\\nconstraints and now we can just use clip\\nand get pretty impressive results very\\nquickly all it requires is a bit of code\\nchanging here and there now I think clip\\nis one part of a trend in multimodality\\nthat is kind of creating a more\\naccessible ml that is less brittle like\\nmodels were in the past that required a\\nlot of fine tuning just to adapt to a\\nslightly different domain and just more\\ngenerally applicable which I thing is\\nreally exciting and I'm I'm it's really\\ncool to see this sort of thing actually\\nbeing used and to actually use it and\\njust see how easy it is to use clip for\\nso many different use cases and it work\\nlike incredibly easily so that's it for\\nthis video I hope it has been useful\\nso thank you very much for watching and\\nI will see you again in the next one bye\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8255114298954732), NodeWithScore(node=TextNode(id_='244cd470-997b-43da-8fb4-3d3a925d8ae2', embedding=None, metadata={'video_id': 'i3OYlaoj-BM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='43db68bf-51a9-4396-bd92-a9c0859f021c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'i3OYlaoj-BM'}, hash='41a8b6cb1eac4d98a00cec03a94ef4b03386e74e3c5f3144504b87d29f265346'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='8ad20062-4699-4e04-ad71-2c9afb1b7318', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'i3OYlaoj-BM'}, hash='b98a05c9ddb1c489938391fc419f667efc0281eb9b2de63b33b52cc31c6ac764'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='007a28d4-1358-4879-93d1-f11a2380575a', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'i3OYlaoj-BM'}, hash='47f23d5c814317565adf3dd840259fecf0fd18da1abf7a4e90de188254eab6b7')}, hash='29484b51c86703c033bd619d2198d156a8a6720a644ece34da672006c59821b3', text=\"Source 5:\\nunderstanding\\nof all these different things and we can\\neven adjust the task being performed\\nwith just a few code changes we don't\\nactually have to is the model itself we\\njust adjust the code around the model\\nand that's very much thanks to Clips\\nfocus on sort of comparing these vectors\\nso for example for classification we\\ngive clip a list of our plus labels and\\nthen we pass in the images and we just\\nidentify within that space where those\\nimages are with respect to those plus\\nlabel vectors and which plus label is\\nthe most similar to our particular image\\nand then that is our prediction so that\\nmost similar plus label\\nthat's our predated class now for\\nobjects localization we apply a very\\nsimilar type of logic as before we\\ncreate a class label but unlike before\\nwe don't need the entire image into clip\\nto localize an object we have to break\\nthe image into patches we then pass a\\nwindow over all of those patches moving\\nacross the entire image left to right\\ntop to bottom and we generate a image\\nembedding for each of those windows and\\nthen we calculate the similarity between\\neach one of those windows embedded by\\nclip and the class label embedding\\nreturning a similarity score for every\\nsingle patch now after calculating the\\nsimilarity score for every single patch\\nwe use that to create almost like a map\\nof relevance across the entire image and\\nthen we can use that map to identify the\\nlocation of the object of interest and\\nfrom that we will get something that's\\nkind of like this so we have most of the\\nimage will be very dark and black that\\nmeans as the object of interest is not\\nin that space and then using that\\nlocalization map we can create a more\\ntraditional bounding box visualization\\nas well both of these visuals are\\ncapturing the same information we're\\njust displaying it in a different way\\nnow there's also other approaches to\\nthis so I recently hosted a talk with\\nwhat two two sets of people actually so\\nFederico Bianchi from Stanford's NLP\\ngroup and also Raphael pissoni and both\\nof those have worked on a Italian clip\\nproject and part of that was performing\\nobject localization now to do that they\\nuse a slightly different approach what\\nI'm going to demonstrate here and we can\\nthink of it as almost like the opposite\\nso whereas we slide a window over the\\nwhole image they slide a black patch\\nover the whole\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8090677877913656), NodeWithScore(node=TextNode(id_='244cd470-997b-43da-8fb4-3d3a925d8ae2', embedding=None, metadata={'video_id': 'i3OYlaoj-BM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='43db68bf-51a9-4396-bd92-a9c0859f021c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'i3OYlaoj-BM'}, hash='41a8b6cb1eac4d98a00cec03a94ef4b03386e74e3c5f3144504b87d29f265346'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='8ad20062-4699-4e04-ad71-2c9afb1b7318', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'i3OYlaoj-BM'}, hash='b98a05c9ddb1c489938391fc419f667efc0281eb9b2de63b33b52cc31c6ac764'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='007a28d4-1358-4879-93d1-f11a2380575a', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'i3OYlaoj-BM'}, hash='47f23d5c814317565adf3dd840259fecf0fd18da1abf7a4e90de188254eab6b7')}, hash='29484b51c86703c033bd619d2198d156a8a6720a644ece34da672006c59821b3', text=\"Source 6:\\nwhereas we slide a window over the\\nwhole image they slide a black patch\\nover the whole image which hides what is\\nbehind in that patch and then they feed\\nthe image into click and essentially as\\nyou slide the patch over the image you\\nare hiding a part of the image and\\ntherefore if this similarity score drops\\nwhen the patch is over a certain area\\nyou know that the object you're looking\\nfor is probably within that space and\\nthat's called the occlusion algorithm\\nnow moving on to object detection which\\nis like the last level in these three\\ntasks we will be identifying multiple\\nobjects now there's a very fine line\\nbetween object localization and object\\ndetection but you can simply think of it\\nas localization for multiple clusters\\nand multiple objects with our cat and\\nButterfly image we will be searching for\\ntwo objects a cat and a butterfly and\\nwith that we could draw a bounding box\\naround both of those objects and\\nessentially what we're doing now is\\nusing localization for a single object\\nbut then we're putting both of those\\ntogether in a loop in our code and we're\\nproducing this object detection process\\nnow we've covered the idea behind my\\nimage classification onto object\\nlocalization and object detection now\\nlet's have a look at how we actually\\nImplement all of this now before we move\\non to any classification localization or\\ndetection task we need to have some data\\nwe're going to use a small demo data set\\ncalled James Callum image text demo and\\nwe can download it like this so we're\\nusing hooking phase data sets here which\\nwe can pip install with Pip install\\ndata sets\\nand this is the day so it's very small\\nit's 21 text to image pairs okay one of\\nthose is the image you've already seen\\nthe cat with a butterfly landing on its\\nnose very curious how they got that\\nphoto now after you've downloaded that\\ndata set\\nwe're going to be using this image here\\nand what we want to do is not use the\\nimage file itself because at the moment\\nit's a it's a pill python image object\\nbut instead we need to convert it into a\\ncanister now we're going to be using pi\\ntorch later on so what I want to do here\\nis we're going to just transform the\\nimage into a tensor and we use toxin\\ntransforms use the typical pipeline tool\\nin computer\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8090677877913656), NodeWithScore(node=TextNode(id_='244cd470-997b-43da-8fb4-3d3a925d8ae2', embedding=None, metadata={'video_id': 'i3OYlaoj-BM'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='43db68bf-51a9-4396-bd92-a9c0859f021c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'video_id': 'i3OYlaoj-BM'}, hash='41a8b6cb1eac4d98a00cec03a94ef4b03386e74e3c5f3144504b87d29f265346'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='8ad20062-4699-4e04-ad71-2c9afb1b7318', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'i3OYlaoj-BM'}, hash='b98a05c9ddb1c489938391fc419f667efc0281eb9b2de63b33b52cc31c6ac764'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='007a28d4-1358-4879-93d1-f11a2380575a', node_type=<ObjectType.TEXT: '1'>, metadata={'video_id': 'i3OYlaoj-BM'}, hash='47f23d5c814317565adf3dd840259fecf0fd18da1abf7a4e90de188254eab6b7')}, hash='29484b51c86703c033bd619d2198d156a8a6720a644ece34da672006c59821b3', text='Source 7:\\nthe\\nimage into a tensor and we use toxin\\ntransforms use the typical pipeline tool\\nin computer vision and we just use tube\\ntensor\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8090677877913656)], metadata={'8ad20062-4699-4e04-ad71-2c9afb1b7318': {'video_id': 'i3OYlaoj-BM'}, 'b40c56d0-61d6-41a6-9110-cf1c7628ce82': {'video_id': 'i3OYlaoj-BM'}, '244cd470-997b-43da-8fb4-3d3a925d8ae2': {'video_id': 'i3OYlaoj-BM'}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Zero shot object detection refers to the ability of a model to detect and localize objects in a new domain without the need for fine-tuning on data from that domain [1]. This means that the model can be applied to a new domain without any prior training on data from that domain [1].'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Source 1: the imaginative laws scale visual recognition challenge was a world-changing competition that ran from around 2010 to 2017. during his time the competition acted as the place to go if you needed to find what the current state of the art was in image classification object localization object detection as well as that 2012 onwards it really acted as the Catalyst of the explosion in deep learning researchers fine-tuned better performing computer vision models year on year but there was a unquestioned assumption causing problems it was assumed that every new task required model fine tuning this required a lot of data and a lot of data required a lot of capital and time it wasn't until recently that this assumption was challenged and proven wrong the astonishing rise of what are called multi-modal models has made the what was thought impossible very possible across various domains and tasks one of those is called zero shot object detection and localization now zero shot refers to taking a model and applying it to a new domain without ever fine-tuning it on data from that new domain so that means we can take a model who can it maybe it works in in one domain classification in one particular area on one data set and we can take that same model without any fine tuning and we can use it for object detection in a completely different domain without that model seeing any training data from that new domain so in this video we're going to explore how to use open ai's clip for zero shots object detection and localization let's begin with taking a quick look at image classification now image classification can kind of be seen as one of the simplest tasks in visual recognition and it's also the first step on the way to object detection at his core it's just assigning a categorical label to an image now moving on from image classification we have object localization object localization is image classification followed by the identification of where in the image the specific object actually is so we are localizing the the object now doing that where essentially just going to identify the coordinates on the image I going to return the typical approach to this is return an image where you have like a bounding box surrounding the the object that you are looking for and then we take this one step further to perform object detection with detection we \""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.source_nodes[0].node.get_text().replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['video_id:', 'i3OYlaoj-BM']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.source_nodes[0].node.get_metadata_str().split()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thirteenlabs-6f7q-2NZ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
